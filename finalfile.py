# -*- coding: utf-8 -*-
"""FinalFile.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1u9WuOdjOY1lD3Vn1MYdeoJgit0ILSzLm
"""

from pydrive.auth import GoogleAuth
from pydrive.drive import GoogleDrive
from google.colab import auth
from oauth2client.client import GoogleCredentials
from sklearn.preprocessing import MinMaxScaler
from sklearn.model_selection import train_test_split
from sklearn.multiclass import OneVsRestClassifier
from sklearn.svm import SVC
from sklearn.multiclass import OneVsRestClassifier
from sklearn.model_selection import StratifiedKFold, GridSearchCV
from sklearn.utils import class_weight as cw, resample
import random

from google.colab import drive
drive.mount('/content/gdrive')

import os
import xlrd
all_files_loc  = r'/content/gdrive/My Drive/DATASET'
all_files = os.listdir(all_files_loc)

Features ="Min, Max, STD, Mean, Median, Activity, Mobility, Complexity, Kurtosis, 2nd Difference Mean, 2nd Difference Max, 1st Difference Mean, 1st Difference Max, Coeffiecient of Variation, Skewness, Wavelet Approximate Mean, Wavelet Approximate Std Deviation, Wavelet Detailed Mean, Wavelet Detailed Std Deviation, Wavelet Approximate Energy, Wavelet Detailed Energy, Wavelet Approximate Entropy, Wavelet Detailed Entropy, Mean of Vertex to Vertex Slope, Var of Vertex to Vertex Slope, FFT Delta Max Power, FFT Theta Max Power, FFT Alpha Max Power, FFT Beta Max Power, Delta/Alpha, Delta/Theta"
features=Features.split(", ")

chs = ['FP1','FPZ','FP2','AF3','AF4','F7','F5','F3','F1','FZ','F2','F4','F6','F8','FT7','FC5','FC3','FC1','FCZ','FC2','FC4','FC6','FT8','T7','C5','C3','C1','CZ','C2','C4','C6','T8','M1','TP7','CP5','CP3','CP1','CPZ','CP2','CP4','CP6','TP8','M2','P7','P5','P3','P1','PZ','P2','P4','P6','P8','PO7','PO5','PO3','POZ','PO4','PO6','PO8','O1','OZ','O2']

import pandas as pd
import numpy as np
wb = xlrd.open_workbook(all_files_loc+'/Data_4_Import_REST.xlsx')
sheet = wb.sheet_by_index(0)
allSeshRows=list()
for i in range(1,sheet.nrows):
    row = sheet.row_values(i)
    allSeshRows.append(row[6])

fullDataset=pd.DataFrame()

unwanted_numbers=[522,581,539,571,572,613]
subject_IDs=np.arange(507, 628, 1).tolist()
subject_IDs = [num for num in subject_IDs if num not in unwanted_numbers]

labels=pd.DataFrame(allSeshRows,index=subject_IDs)

for filename in all_files :
  if filename.endswith('.npy'):
      data1 = np.load(all_files_loc+'/'+filename, allow_pickle=True)
      df0 = pd.DataFrame(data1[0][0], columns=features, index=chs)
      df = df0.reset_index().copy()
      df.rename(columns={'index': 'channel'}, inplace=True)
      test = df.loc[df['channel'] == 'T8']
      test = test.drop(['channel'], axis=1)
      fullDataset = fullDataset.append(test.iloc[:][:],ignore_index=True)

fullDataset=fullDataset[['1st Difference Max','Wavelet Detailed Energy', 'Var of Vertex to Vertex Slope', 'Wavelet Detailed Std Deviation','Delta/Alpha', 'Delta/Theta','Kurtosis']]

scaler = MinMaxScaler()
fullDataset.index=subject_IDs
fullDataset=fullDataset.reindex(subject_IDs)
fullDataset[:]=scaler.fit_transform(fullDataset[:])

fullDataset=fullDataset[['1st Difference Max','Wavelet Detailed Energy', 'Var of Vertex to Vertex Slope', 'Wavelet Detailed Std Deviation','Delta/Alpha', 'Delta/Theta','Kurtosis']]
fullDataset= fullDataset.assign(BDI=labels[0])

fullDataset['BDI']=pd.cut(fullDataset['BDI'],bins=[0,13.0,19.0,28.0,63.0],labels=[0,1,2,3])
fullDataset['BDI'] = fullDataset['BDI'].fillna(0)

def upsample(classNum,sampleSize):
 return resample(classNum,
  replace=True,    
  n_samples=sampleSize,     
  random_state=123)

class0= fullDataset[fullDataset.BDI==0]
class1= fullDataset[fullDataset.BDI==1]
class2= fullDataset[fullDataset.BDI==2]
class3= fullDataset[fullDataset.BDI==3]
majority_count=len(class0)

upsampled_class1=upsample(class1,majority_count)
upsampled_class2=upsample(class2,majority_count)
upsampled_class3=upsample(class3,majority_count)

print(len(class0))
print(len(upsampled_class1))
print(len(upsampled_class2))
print(len(upsampled_class3))

upsampledData=pd.concat([class0,upsampled_class1,upsampled_class2,upsampled_class3])

X=upsampledData[['1st Difference Max','Wavelet Detailed Energy', 'Var of Vertex to Vertex Slope', 'Wavelet Detailed Std Deviation','Delta/Alpha', 'Delta/Theta','Kurtosis']]
y=upsampledData[['BDI']]

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, shuffle=True)

params_grid = [{'C': [1, 10, 100, 1000], 'gamma': [0.001, 0.0001], 'kernel': ['rbf']}]

svm=GridSearchCV(estimator=SVC(kernel='rbf', probability=True), param_grid=params_grid, cv=5,return_train_score=True)
fitted=svm.fit(X_train, y_train.values.ravel())
scores=fitted.cv_results_

classifier = OneVsRestClassifier(svm.best_estimator_)
fitted=classifier.fit(X_train, y_train)

predicted = classifier.predict(X_test)
predicted_probability = classifier.predict_proba(X_test)

from sklearn.metrics import classification_report
target_names=['minimal range','mild','moderate','severe']
print(classification_report(y_test,predicted,target_names=target_names))

from sklearn.metrics import confusion_matrix

confMat=confusion_matrix(y_true=y_test,y_pred=predicted,labels=[0,1,2,3])

print(confMat)

total=sum(sum(confMat))
TP=confMat[0,0]
FP=confMat[1,0]
FN=confMat[0,1]
TN=confMat[1,1]
sensitivity = TP/(TP+FP)
print('Sensitivity : ', sensitivity )
specificity = TN/(FP+TN)
print('specificity : ', specificity )

predicted_probability=pd.DataFrame(predicted_probability)
predicted_probability.columns=['probability of class1','probability of class2','probability of class3','probability of class4']